{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cee8351-eafa-4791-ba31-46b1d97df904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,precision_score, accuracy_score, confusion_matrix, classification_report, recall_score, f1_score\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, FixedLocator)\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf_RF = RandomForestRegressor(max_depth=400, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba7558b-afc9-4271-a64c-7990ec8ccba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init=\"May\" # forecast \"initialisation\" month\n",
    "trg=\"MJJ\" # forecast target month\n",
    "\n",
    "dataset=Dataset(\"past2k_tasmax_HWs_EUR_MJJA_period70018850_clim88218850.nc\",'r')\n",
    "lons=dataset['lon'][:]\n",
    "lats=dataset['lat'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a727f0-ef46-4941-96a2-151a57de25f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of grid points:  1\n"
     ]
    }
   ],
   "source": [
    "# Solutions files for each grid point #\n",
    "files=sorted(glob.glob(\"opt_MayMJJ_past2k_*.csv\"))\n",
    "print (\"Number of grid points: \",len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a72cf65-c56a-469e-9b2f-0703343be8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Open solutions and extract best solution ###\n",
    "\n",
    "# Lists for best solutions #\n",
    "xs=[] # x-coord\n",
    "ys=[] # y-coord\n",
    "nevals=[] # number of evaluations \n",
    "cv_best=[] # best cross-validation/training score\n",
    "test_best=[] # test score corresponding to cv_best\n",
    "sols_best=[] # predictors correspondin to cv_best\n",
    "\n",
    "for file in files:\n",
    "    #print (file[-9:-4])\n",
    "\n",
    "    label=file[-9:-4].split(\"_\") # identify x-coord and y-coord in file name\n",
    "\n",
    "    sol_file_av = pd.read_csv(file, index_col=None, sep=' ', header=0)#[:20]\n",
    "    if sol_file_av.shape[0]>0:\n",
    "        xs.append(int(label[1]))\n",
    "        ys.append(int(label[0]))\n",
    "        nevals.append(sol_file_av.shape[0])\n",
    "        sols_best.append(np.fromstring(sol_file_av.Sol[sol_file_av.sort_values(by=['CV'],ascending=True).index[0]].replace('[', '').replace(']', '').replace('\\n', ''), dtype=float, sep=' '))\n",
    "        cv_best.append(sol_file_av.CV[sol_file_av.sort_values(by=['CV'],ascending=True).index[0]])\n",
    "        test_best.append(sol_file_av.Test[sol_file_av.sort_values(by=['CV'],ascending=True).index[0]])\n",
    "    else:\n",
    "        print (\"Empty file - no solutions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192cc889-14ef-438a-9a08-1ebb6837e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of past2k to higher resolution ERA5\n",
    "#target_2D=np.concatenate((target_past2k,target_ERA5))\n",
    "\n",
    "# 1 - Input ERA5 lon and lat\n",
    "# 2 - Find nearest past2k point\n",
    "# 3 - Identify p value\n",
    "# 4 - Concatenate past2k and ERA5 HWs\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371 # km\n",
    "    p = pi / 180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 2 * r * asin(sqrt(a))\n",
    "\n",
    "def find_nearest(era_lat,era_lon):\n",
    "    dmin=9E15\n",
    "    dmin_ind=0\n",
    "    #print (era_lat,era_lon)\n",
    "    for x in range(len(xs)):\n",
    "        #print (lats[ys[x]],lons[xs[x]])\n",
    "        d=distance(era_lat,era_lon,lats[ys[x]],lons[xs[x]])\n",
    "        if d<dmin:\n",
    "            dmin=d\n",
    "            dmin_ind=x\n",
    "        #print (d)\n",
    "    return dmin_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea4acad-8756-41a3-af67-c8dbdb24cb46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Reformat optimal prpedictors ###\n",
    "\n",
    "def create_board(n_rows, n_cols, final_sequence, sequence_length, feat_sel):\n",
    "    board = np.zeros((n_rows, n_cols))\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        start_index = int(final_sequence[i]) \n",
    "        end_index = int(final_sequence[i])  + int(sequence_length[i])\n",
    "        if feat_sel[i] != 0:\n",
    "            board[start_index:end_index, i] = 1\n",
    "    \n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf25159-854c-4ab7-87a2-f25f224e5e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Produce forecasts based on optimal predictors ###\n",
    "\n",
    "# Inputs: p - grid point index, mod - ML model, pred_dataframe - full list of predcitors, remove-co2 - boolean to remove CO2 as predictor\n",
    "def forecast(p,mod,pred_dataframe,remove_co2):\n",
    "    fn=find_nearest(lats_era5_tile[p],lons_era5_tile[p])\n",
    "\n",
    "    target=np.concatenate((target_past2k[:,ys[fn],xs[fn]],target_ERA5.reshape(l,target_ERA5[0].size)[:,p]))\n",
    "\n",
    "    array_best=sols_best[fn]\n",
    "\n",
    "    #array_best=sols_best[p]\n",
    "    #target=target_2D[:,ys[p],xs[p]]\n",
    "    \n",
    "    df=pd.DataFrame(target,columns=['Target'])\n",
    "    df.index = target_dates\n",
    "    target_dataset=df\n",
    "\n",
    "    split=int(array_best.shape[0]/3)\n",
    "    \n",
    "    sequence_length_best = array_best[0:split]\n",
    "    final_sequence_best = array_best[split:split*2]\n",
    "    feat_sel_best = array_best[split*2:]\n",
    "\n",
    "    if remove_co2:\n",
    "        sequence_length_best=np.delete(sequence_length_best, -2)\n",
    "        final_sequence_best=np.delete(final_sequence_best, -2)\n",
    "        feat_sel_best=np.delete(feat_sel_best, -2)\n",
    "        n_cols = split-1\n",
    "    else:\n",
    "        n_cols = split\n",
    "    n_rows = int((sequence_length_best + final_sequence_best).max())+10\n",
    "    \n",
    "    board_best = create_board(n_rows, n_cols, final_sequence_best, sequence_length_best, feat_sel_best)\n",
    "\n",
    "    time_lags = np.repeat(0,pred_dataframe.shape[1])\n",
    "    time_sequences = np.repeat(n_rows,n_cols)\n",
    "    variable_selection = np.repeat(1,pred_dataframe.shape[1])\n",
    "    \n",
    "    dataset_opt = target_dataset.copy()\n",
    "    for i,col in enumerate(pred_dataframe.columns):      \n",
    "        if remove_co2:\n",
    "            if col==\"data_CO2\":\n",
    "                continue\n",
    "        if col==\"weekofyear\":\n",
    "            i=i-1\n",
    "        if variable_selection[i] == 0:\n",
    "            continue\n",
    "        for j in range(time_sequences[i]):\n",
    "            dataset_opt[str(col)+'_lag'+str(time_lags[i]+j)] = pred_dataframe[col].shift(time_lags[i]+j)\n",
    "\n",
    "        \n",
    "    first_train_index=int(np.argwhere(df.index==first_train))\n",
    "    last_train_index=int(np.argwhere(df.index==last_train))\n",
    "    \n",
    "    train_dataset_opt = dataset_opt[first_train_index:last_train_index]\n",
    "    test_dataset_opt = dataset_opt[last_train_index:]\n",
    "\n",
    "    Y_column = 'Target' \n",
    "           \n",
    "    X_train=train_dataset_opt[train_dataset_opt.columns.drop([Y_column]) ]\n",
    "    Y_train=train_dataset_opt[Y_column]\n",
    "    X_test=test_dataset_opt[test_dataset_opt.columns.drop([Y_column]) ]\n",
    "    Y_test=test_dataset_opt[Y_column]\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_std_train = scaler.fit(X_train)\n",
    "\n",
    "    X_std_train = scaler.transform(X_train)\n",
    "    X_std_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train=pd.DataFrame(X_std_train,columns=X_train.columns,index=X_train.index)\n",
    "    X_test=pd.DataFrame(X_std_test,columns=X_test.columns,index=X_test.index)\n",
    "    \n",
    "    X_train_new = np.array(X_train)[:,board_best.T.reshape(1,-1)[0].astype(bool)]\n",
    "    X_test_new = np.array(X_test)[:,board_best.T.reshape(1,-1)[0].astype(bool)]\n",
    "\n",
    "    clf = mod\n",
    "    clf.fit(X_train_new, Y_train)\n",
    "    predictions = clf.predict(X_test_new)\n",
    "    Y_pred = pd.DataFrame(predictions, columns=['Y_pred'], index=Y_test.index)\n",
    "    \n",
    "    return clf.__class__.__name__,predictions # full model name, 1D output forecast (year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828ee6f9-63d1-40fa-ae7b-cd19ee6858e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop \"forecast\" function through all points ###\n",
    "# Input: clf - model, l - length in years of test period\n",
    "def predout(clf,l,pred_dataframe,remove_co2):\n",
    "    preds=np.zeros((len(xs),l)) #(point, year)\n",
    "\n",
    "    for point in range(len(xs)):\n",
    "        #print (point)\n",
    "        output=forecast(point,clf,pred_dataframe,remove_co2)\n",
    "        preds[point]=output[1]\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e1d4ec-6090-48ad-b876-116c23019092",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save output of forecast/predout ###\n",
    "def saver(preds,mod,l):\n",
    "    fout=Dataset(\"Forecasts_initMay_targMJJ_{2}_19792021.nc\".format(init,trg,mod), \"w\")\n",
    "\n",
    "    lat = fout.createDimension('x', len(xs))\n",
    "    day = fout.createDimension('year', l) \n",
    "\n",
    "    var = fout.createVariable('lat', int, ('x')) \n",
    "    var.standard_name = \"Latitude\"\n",
    "    var[:]=ys\n",
    "    var = fout.createVariable('lon', int, ('x')) \n",
    "    var.standard_name = \"Longitude\"\n",
    "    var[:]=xs\n",
    "\n",
    "    var = fout.createVariable('pred', int, ('x','year')) \n",
    "    var.standard_name = \"NDQ90 predictions\"\n",
    "    var[:]=np.array(preds)\n",
    "\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021940da-aa59-47c1-9e25-323afae46af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Open HW target dataset ###\n",
    "\n",
    "dataset=Dataset(\"past2k_tasmax_HWs_EUR_MJJA_period70018850_clim88218850.nc\",'r')\n",
    "target_past2k=dataset['NDQ90_May'][:,0,:,:]+dataset['NDQ90_Jun'][:,0,:,:]+dataset['NDQ90_Jul'][:,0,:,:]\n",
    "\n",
    "dataset=Dataset(\"ERA5_tmax_HWs_EUR_NDQ90_period19402022_clim19932016.nc\",'r') \n",
    "target_ERA5=dataset['NDQ90_May'][1979-1940:2021-1940,0,:,:]+dataset['NDQ90_Jun'][1979-1940:2021-1940,0,:,:]+dataset['NDQ90_Jul'][1979-1940:2021-1940,0,:,:]\n",
    "lons_ERA5=dataset['lon'][:]\n",
    "lats_ERA5=dataset['lat'][:]\n",
    "\n",
    "lons_era5_tile=np.tile(lons_ERA5,lats_ERA5.size)\n",
    "lats_era5_tile=np.tile(lats_ERA5,lons_ERA5.size)\n",
    "\n",
    "#target_2D=np.concatenate((target_past2k,target_ERA5))\n",
    "   \n",
    "target_dates=[] # dummy date for summer HWMI\n",
    "train_years_past2k=range(7001,8851,1)\n",
    "train_years_era5=range(1979,2021,1)\n",
    "for year in train_years_past2k:\n",
    "    target_dates.append(str(year).zfill(4)+\"-04-30\") # Days in June #\n",
    "for year in train_years_era5:\n",
    "    target_dates.append(str(year).zfill(4)+\"-04-30\") # Days in June #\n",
    "\n",
    "#===============================#\n",
    "\n",
    "first_train = \"7002-04-30\"\n",
    "last_train = \"1979-04-30\"\n",
    "\n",
    "pred_dataframe_era5 = pd.read_csv('Predictors_dataset_ERA5_weekly.csv', index_col=0)\n",
    "\n",
    "pred_dataframe_past2k = pd.read_csv('Predictors_dataset_past2k_weekly.csv', index_col=0)\n",
    "\n",
    "pred_dataframe=pd.concat([pred_dataframe_past2k,pred_dataframe_era5])\n",
    "\n",
    "# Convert ERA5 predictor to past2k units\n",
    "# Soil Moisture kg/m2 , ERA5 - m3/s3 (divide by 0.1m, divide by 1000 kg.m3, times by 0.7 = divide by 70)\n",
    "pred_dataframe['smEurope_cluster1']['1979-01-01':]=(pred_dataframe['smEurope_cluster1']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster2']['1979-01-01':]=(pred_dataframe['smEurope_cluster2']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster3']['1979-01-01':]=(pred_dataframe['smEurope_cluster3']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster4']['1979-01-01':]=(pred_dataframe['smEurope_cluster4']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster5']['1979-01-01':]=(pred_dataframe['smEurope_cluster5']['1979-01-01':].values)*70\n",
    "\n",
    "# SIC Arctic\n",
    "# past2k - percentage , ERA5 - proportion \n",
    "pred_dataframe['sicArctic_cluster1']['1979-01-01':]=pred_dataframe['sicArctic_cluster1']['1979-01-01':].values*100\n",
    "pred_dataframe['sicArctic_cluster2']['1979-01-01':]=pred_dataframe['sicArctic_cluster2']['1979-01-01':].values*100\n",
    "pred_dataframe['sicArctic_cluster3']['1979-01-01':]=pred_dataframe['sicArctic_cluster3']['1979-01-01':].values*100\n",
    "pred_dataframe['sicArctic_cluster4']['1979-01-01':]=pred_dataframe['sicArctic_cluster4']['1979-01-01':].values*100\n",
    "pred_dataframe['sicArctic_cluster5']['1979-01-01':]=pred_dataframe['sicArctic_cluster5']['1979-01-01':].values*100\n",
    "\n",
    "l=42 # (length of period: 1950-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb052eb3-73f9-4b06-af78-90b0060bea7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_co2=True\n",
    "\n",
    "preds=predout(clf_RF,l,pred_dataframe,remove_co2)\n",
    "saver(preds,\"RF\",l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ff730-75d2-4a4e-af54-b6a55c43f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed3cda-3d34-4e2c-acd4-18194bcba6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML (based on the latest module pytorch)",
   "language": "python",
   "name": "ml-aim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
