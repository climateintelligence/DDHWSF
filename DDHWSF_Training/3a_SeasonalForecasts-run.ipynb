{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e8ccc7-c5fe-4fca-a66f-d8fb946ec8a0",
   "metadata": {},
   "source": [
    "# NOTEBOOK 3A - Run Seasonal Forecasts.\n",
    "### This script uses the selected features (from the best solution of the optimisation algorithm).\n",
    "### ERA5 predictors are used to train ML models (.e.g Random Forest).\n",
    "### Output: forecasts of the target (NDQ90) over the 1993-2016 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cee8351-eafa-4791-ba31-46b1d97df904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../Modules/')\n",
    "from seasonal_forecasts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e05f2-de8d-448c-913a-506f88ffb1f5",
   "metadata": {},
   "source": [
    "## STEP 1 - Choose ML model.\n",
    "### Select your model (clf_) from: Linear Regression (LR), Support Vector (SVR), Decision Tree (DT), Random Forest (RF), K Nearest Neighbour (neigh), \n",
    "### AdaBoost (AB), Multi-Layer Perceptron (MLP), Light Gradient Boost (LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba7558b-afc9-4271-a64c-7990ec8ccba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ML_models_regressors import clf_LR, clf_SVR, clf_DT, clf_RF, clf_neigh, clf_AB, clf_MLP, clf_LGBM\n",
    "clf=clf_LR # clf_RF, clf_AB etc...\n",
    "mod=\"LR\" # change name to save file\n",
    "\n",
    "output_file = f\"Output/DDHWSF_Forecasts_{mod}_19792021.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7928106-bcbe-4b28-a4db-ca1bdef2489e",
   "metadata": {},
   "source": [
    "## STEP 2 - Extract Optimal Predictors.\n",
    "### Solutions files for each grid point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a727f0-ef46-4941-96a2-151a57de25f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files=sorted(glob.glob(\"Output/optimisation_output.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a72cf65-c56a-469e-9b2f-0703343be8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nevals=[] # number of evaluations \n",
    "cv_best=[] # best cross-validation/training score\n",
    "test_best=[] # test score corresponding to cv_best\n",
    "sols_best=[] # predictors correspondin to cv_best\n",
    "\n",
    "for file in files:\n",
    "    #print (file[-9:-4])\n",
    "    sol_file_av = pd.read_csv(file, index_col=None, sep=' ', header=0)#[:20]\n",
    "    if sol_file_av.shape[0]>0:\n",
    "        nevals.append(sol_file_av.shape[0])\n",
    "        sols_best.append(np.fromstring(sol_file_av.Sol[sol_file_av.sort_values(by=['CV'],ascending=True).index[0]].replace('[', '').replace(']', '').replace('\\n', ''), dtype=float, sep=' '))\n",
    "        cv_best.append(sol_file_av.CV[sol_file_av.sort_values(by=['CV'],ascending=True).index[0]])\n",
    "        test_best.append(sol_file_av.Test[sol_file_av.sort_values(by=['CV'],ascending=True).index[0]])\n",
    "    else:\n",
    "        print (\"Empty file - no solutions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61025862-b60c-489f-949c-c0f0d5718020",
   "metadata": {},
   "source": [
    "## STEP 3 - Open Target Data.\n",
    "### Training on past2k (0-1850)\n",
    "### Testing on ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "021940da-aa59-47c1-9e25-323afae46af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Open HW target dataset ###\n",
    "\n",
    "df1 = pd.read_csv(\"Output/NumberHWdays_past2k_Cluj-Napoca_thresh90_dur3.csv\")\n",
    "target_past2k=df1.NumberHWDays\n",
    "\n",
    "df2 = pd.read_csv(\"Output/NumberHWdays_ERA5_Cluj-Napoca_thresh90_dur3.csv\")\n",
    "target_ERA5=df2.NumberHWDays#[1993-1993:2021-1940]\n",
    "\n",
    "#===============================#\n",
    "\n",
    "pred_dataframe_era5 = pd.read_csv('Predictors_dataset_ERA5_weekly-smallsample.csv', index_col=0)\n",
    "\n",
    "pred_dataframe_past2k = pd.read_csv('Predictors_dataset_past2k_weekly-smallsample.csv', index_col=0)\n",
    "pred_dataframe=pd.concat([pred_dataframe_past2k,pred_dataframe_era5])\n",
    "\n",
    "# Convert ERA5 predictor to past2k units\n",
    "# Soil Moisture kg/m2 , ERA5 - m3/s3 (divide by 0.1m, divide by 1000 kg.m3, times by 0.7 = divide by 70)\n",
    "pred_dataframe['smEurope_cluster1']['1979-01-01':]=(pred_dataframe['smEurope_cluster1']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster2']['1979-01-01':]=(pred_dataframe['smEurope_cluster2']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster3']['1979-01-01':]=(pred_dataframe['smEurope_cluster3']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster4']['1979-01-01':]=(pred_dataframe['smEurope_cluster4']['1979-01-01':].values)*70\n",
    "pred_dataframe['smEurope_cluster5']['1979-01-01':]=(pred_dataframe['smEurope_cluster5']['1979-01-01':].values)*70\n",
    "\n",
    "# SIC Arctic\n",
    "# past2k - percentage , ERA5 - proportion \n",
    "# !!! Uncomment if using all predictors !!!\n",
    "#pred_dataframe['sicArctic_cluster1']['1979-01-01':]=pred_dataframe['sicArctic_cluster1']['1979-01-01':].values*100\n",
    "#pred_dataframe['sicArctic_cluster2']['1979-01-01':]=pred_dataframe['sicArctic_cluster2']['1979-01-01':].values*100\n",
    "#pred_dataframe['sicArctic_cluster3']['1979-01-01':]=pred_dataframe['sicArctic_cluster3']['1979-01-01':].values*100\n",
    "#pred_dataframe['sicArctic_cluster4']['1979-01-01':]=pred_dataframe['sicArctic_cluster4']['1979-01-01':].values*100\n",
    "#pred_dataframe['sicArctic_cluster5']['1979-01-01':]=pred_dataframe['sicArctic_cluster5']['1979-01-01':].values*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edb946-1dcc-4fd0-94a3-b4088bef2bd9",
   "metadata": {},
   "source": [
    "## STEP 4 - Run Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb052eb3-73f9-4b06-af78-90b0060bea7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LinearRegression', array([ 6.63195152,  5.60165792, -0.96387317,  4.96850735,  8.64470351,\n",
      "       10.0265519 , 13.54700773, 11.56180065,  9.05766582, 12.67262078,\n",
      "       15.88298215,  9.1400354 , 11.04539333,  8.11544981, 14.65144931,\n",
      "       13.67596433, 12.87135144,  9.82562174,  7.90400809, 14.25149241,\n",
      "       12.0614981 ,  9.95268653, 18.56503409, 10.40540433, 13.71085124,\n",
      "       13.9295002 , 14.36012249, 14.90937391]))\n",
      "Saved predictions with metadata to Output/DDHWSF_Forecasts_LR_19792021.csv\n"
     ]
    }
   ],
   "source": [
    "remove_co2=True\n",
    "\n",
    "preds=forecast(target_past2k, target_ERA5, [1993,2020], sols_best[0], clf, pred_dataframe, remove_co2=True)\n",
    "print (preds)\n",
    "saver(output_file,preds[1],target_ERA5.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed3cda-3d34-4e2c-acd4-18194bcba6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
